{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instant-heath",
   "metadata": {},
   "source": [
    "# MSDM5058 Tutorial 1 (Part 1) - Review on probability and statistics\n",
    "\n",
    "## Contents\n",
    "1. Probability of events\n",
    "2. Probability disbtribution\n",
    "3. Statistical measures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-whale",
   "metadata": {},
   "source": [
    "# 1. Probability of events\n",
    "\n",
    "The probability of an event $A$ is denoted as $P(A)$. The two key principles of probability are\n",
    "\n",
    "- $0<P<1$: The probability of any events are values between $0$ and $1$.\n",
    "- $\\sum P = 1$: Sum of the probability of all _mutually exclusive_ events is $1$.\n",
    "\n",
    "\n",
    "## 1.1. Relations between events\n",
    "\n",
    "You should bear in mind these relations between events:\n",
    "\n",
    "### 1.1.1. Complement\n",
    "\n",
    "The complement of an event is the probability that the event dose not occur. It is denoted as $P(\\bar{A})=1-P(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-arlington",
   "metadata": {},
   "source": [
    "### 1.1.2. Intersection\n",
    "\n",
    "The intersection between two events is equivalent to the \"AND\" operation, i.e. the probability for event $A$ AND event $B$ happening together, denoted as $P(A\\cap B)$. It is also called their **joint probability**. \n",
    "\n",
    "- **Independence**: If the occurrence of $A$ and $B$ do not influence each other, they are independent and satisfies the so-called \"product rule\": \n",
    " \n",
    " $$P(A\\cap B) \\stackrel{\\text{indep.}}= P(A) P(B)\\ .$$\n",
    "\n",
    " However, \"independence\" may get philosophically hard to determine. (Does a butterfly in Brazil cause a tornado in the United States?) Hence, as long as his data supports the equality, one may hedge to say that two events are statistically independent instead.\n",
    "\n",
    "- **Mutually exclusive**: If $A$ and $B$ never happen together, they are called mutually exclusive and $P(A\\cap B)\\stackrel{\\text{m.e.}}=0$. A simple example of mutually exclusive events is to get a head and get a tail from flipping one coin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-forward",
   "metadata": {},
   "source": [
    "### 1.1.3. Union\n",
    "\n",
    "The union between two events is equivalent to the \"OR\" operation, i.e. the probability for at least one of $A$ OR $B$ happening, denoted as $P(A\\cup B)$. \n",
    "\n",
    "- **Sum rule**: The direct calculation to union is usually confusing. So we usually make use of the following rules. For two events, it can be calculated as: \n",
    "\n",
    " $$P(A\\cup B) = P(A)+P(B)-P(A\\cap B)\\ .$$\n",
    "\n",
    " It may be generalized as the **inclusion-exclusion principle**: given a set of events $\\{E_1, E_2, E_3,...\\}$, the probability for at least one to happen is\n",
    "\n",
    " $$\n",
    "\\begin{align*}\n",
    "P\\left(\\bigcup E_i\\right) &= \\sum_iP(E_i) - \\sum_{i<j}P(E_i\\cap E_j) + \\sum_{i<j<k}P(E_i\\cap E_j\\cap E_k) + ... \\\\\n",
    "&= \\sum_k\\left[(-1)^{k-1}\\sum_{i_1<...<i_k}P\\left(\\bigcap_{j=1}^{k}E_{i_j}\\right)\\right]\\ .\n",
    "\\end{align*}\n",
    " $$\n",
    "\n",
    "- If the events are independent, the formula can be simplified into \n",
    "    \n",
    "    $$P\\left(\\bigcup E_i\\right) \\stackrel{\\text{indep.}}= 1-\\prod_i[1-P(E_i)] \\ .$$\n",
    "\n",
    "- If the events are mutually exclusive, all terms of intersection vanish, so\n",
    "    \n",
    "    $$P\\left(\\bigcup_i E_i\\right) = \\sum_i P(E_i) \\ .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-intersection",
   "metadata": {},
   "source": [
    "### 1.1.4. Conditional probability\n",
    "\n",
    "This conditional probability of $B$ on $A$ is the probability for $B$ to happen given that $A$ has already happened. It is defined as \n",
    "\n",
    "$$\n",
    "P(B|A) = \\frac{P(A\\cap B)}{P(A)}\n",
    "$$\n",
    "\n",
    "- If the events are independent, the occurrence of $A$ does not alter the probability for $B$ to occur at all. So\n",
    " $$P(B|A) \\stackrel{\\text{indep.}}=P(B)$$\n",
    " \n",
    " \n",
    " \n",
    "- If the events are mutually exclusive, they cannot occur consequencely. So $P(A\\cap B)=0$ and\n",
    " $$P(B|A)\\stackrel{\\text{m.e.}}=0$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-vocabulary",
   "metadata": {},
   "source": [
    "## 1.2. Bayes' theorem\n",
    "\n",
    "In science we can distinguish the two variables as independent variable $A$ and dependent variable $B$ by causality, but in mathematics there is no reasons would stop  $A$ and $B$ being symmetric in the formula, we can write:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "P(B|A) = \\dfrac{P(A\\cap B)}{P(A)} \\\\[0.5em]\n",
    "P(B|A) = \\dfrac{P(A\\cap B)}{P(A)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and thus \n",
    "$$\n",
    "P(A|B)P(B) = P(A\\cap B) = P(B|A)(A)\n",
    "$$\n",
    "\n",
    "It comes to the Bayes' theorem which says: regardless of any causality,\n",
    "\n",
    "$$\n",
    "P(B|A) \\equiv \\frac{P(A|B)P(B)}{P(A)}\n",
    "$$\n",
    "\n",
    "We will see more application of the Bayes' theorem in later lectures and tutorials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-volunteer",
   "metadata": {},
   "source": [
    "# 2. Probability distribution\n",
    "\n",
    "We would often like know how a random variable $X$'s probability $P(X=x)$ distributes over all possible $x$.\n",
    "\n",
    "## 2.1. Terminology\n",
    "\n",
    "### 2.1.1. Probability mass function (PMF)\n",
    "\n",
    "If $X$ is a discrete random variable, its probability mass function (PMF) is defined as\n",
    "\n",
    "$$\n",
    "P_X(x) \\equiv P(X=x)\n",
    "$$\n",
    "\n",
    "For example, the PMF of a dice's outcome $D$ is \n",
    "\n",
    "$$\n",
    "p_D(x) = \\begin{cases}\n",
    " \\frac{1}{6} & (x\\in \\{1,...,6\\})\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\\ .\n",
    "$$\n",
    "\n",
    "\n",
    "### 2.1.2. Probability density function (PDF)\n",
    "\n",
    "The definition of PMF fails if $X$ is continuous, because we cannot find an exact real number on the real number line. (There are, informally speaking, \"infinitely many possible real numbers\", so the probability to locate any particular real number goes to zero.) We need to adopt a different but similar concept for continuous random variables - the probability density function (PDF). If $X$ has a PDF $f_X(x)$, the following identity holds:\n",
    "\n",
    "$$\n",
    "\\int_a^b f_X(x)\\mathrm{d}x \\equiv P(a\\leq X\\leq b) \\ .\n",
    "$$\n",
    "\n",
    "- Therefore the product $f_X(x)\\mathrm{d}x$ may be regarded as the probability of observing $X$ in the range $\\left[{x,x+dx}\\right]$. (While $f_X(x)$ alone is not the probability but the **_probability density_**.) \n",
    "\n",
    "- As $X$ definitely lies in $(-\\infty,\\infty)$, we must impose the normalization condition $\\int_{-\\infty}^\\infty f_X(x)dx\\equiv 1$ on a PDF.\n",
    "\n",
    "\n",
    "### 2.1.3. Cumulative distribution function (CDF)\n",
    "\n",
    "The cumulative distribution function (CDF) of $X$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_X(x) &\\equiv P(X\\leq x) \\\\[0.5em]\n",
    "&= \\begin{cases}\n",
    "\\displaystyle \\sum_{x'\\leq x} p_X(x') & (\\text{discontinous }X) \\\\[0.5em]\n",
    "\\displaystyle \\int_{-\\infty}^x f_X(x')\\mathrm{d}x' & (\\text{continuous }X)\n",
    "\\end{cases}\\ ,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which must satisfy $F(-\\infty)=0$ and $F(+\\infty)=1$. Although it does not bring in new information, CDF is often more useful analytically because of its monotonic nature. For the continuous case, $f_X(x)=\\frac{dF_X(x)}{dx}$ implies that $f_X(\\pm\\infty)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-acquisition",
   "metadata": {},
   "source": [
    "## 2.2. Multivariate distributions\n",
    "\n",
    "We are often interested how a random variable's outcome correlate with others'. In such cases, we need to consider multivariate distributions, the simplest of which contains only two random variables  $X$ and $Y$. While this section focuses on continuous random variables, it is not difficult to rephrase for discrete ones. \n",
    "\n",
    "\n",
    "### 2.2.1. Joint distribution\n",
    "\n",
    "The joint PDF $f_{XY}(x,y)$ is defined to satisfy\n",
    "\n",
    "$$\n",
    "\\int_a^b\\int_c^d f_{XY}(x,y)\\mathrm{d}y\\mathrm{d}x = P(X\\in[a,b] \\cap Y\\in[c,d]) \\ ,\n",
    "$$\n",
    "\n",
    "whereas their joint CDF is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_{XY}(x,y) \\equiv& P(X\\leq x\\cap Y\\leq y) \\\\\n",
    "=& \\int_{-\\infty}^x  \\int_{-\\infty}^y f_{XY}(x', y')\\mathrm{d}y'\\mathrm{d}x' \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Conversely, $f_{XY}(x,y) = \\frac{\\partial^2}{\\partial x\\partial y}F_{XY}$. As the number of variables grows, it becomes more convenient to use the differential form of definition:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_{X_1X_2\\dots X_n}(x_1,x_2,\\dots,x_n)\n",
    "\\equiv& P\\left(\\bigcap_{i=1}^n X_i\\leq x_i\\right) \\\\\n",
    "\\Rightarrow f_{X_1X_2\\dots X_n}(x_1,x_2,\\dots,x_n)\n",
    "\\equiv& \\frac{\\partial^n}{\\partial x_1\\partial x_2\\dots\\partial x_n}F_{X_1X_2\\dots X_n} \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    " \n",
    "\n",
    "### 2.2.2. Marginal distribution\n",
    "\n",
    "Sometimes we are given $f_{XY}(x,y)$, from which we would like to extract $f_X(x)$. Since $P(X=x) =P[X=x\\cap Y\\in (-\\infty, \\infty)]$, we obtain\n",
    "\n",
    "$$\n",
    "\\int_a^b f_X(x') \\mathrm{d}x'\n",
    "= \\int_a^b\\int_{-\\infty}^\\infty f_{XY}(x',y')\\mathrm{d}y'\\mathrm{d}x'\n",
    "$$\n",
    "\n",
    "or simply\n",
    "\n",
    "$$\n",
    "f_X(x) = \\int_{-\\infty}^\\infty f_{XY}(x,y)\\mathrm{d}y \\ .\n",
    "$$\n",
    "\n",
    "This form of deduced PDF is also called a marginal PDF. Similarly, as $P(X\\leq x) =P[X\\leq x \\cap Y\\leq \\infty] $, the marginal CDF of $X$ is\n",
    "\n",
    "$$\n",
    "F_X(x)=F_{XY}(x,y=\\infty) \\ .\n",
    "$$\n",
    "\n",
    "\n",
    "### 2.2.3. Conditional distribution\n",
    "\n",
    "The conditional PDF of $Y$ on $X$ measures the probability density of $Y$ given that $X=x$. It is defined as\n",
    "\n",
    "$$\n",
    "f_{Y|X}(y|x) = \\frac{f_{XY}(x,y)}{f_{X}(x)}\n",
    "$$\n",
    "\n",
    "so that $P(Y\\in [c,d]| X=x) = \\int_c^d f_{Y|X}(y\\mid x) \\mathrm{d}y$. \n",
    "\n",
    "> _(optional reading)_ \n",
    ">\n",
    "> **Informal proof:** \n",
    "> \n",
    "> $$\n",
    "\\begin{align*}\n",
    "P(Y\\in[c,d]|X=x)\n",
    "=& \\lim_{\\delta \\to 0} P(Y\\in [c,d]|X=[x,x+\\delta]) \\\\\\mathrm{d}y\n",
    "=& \\lim_{\\delta \\to 0} \\frac{P(X=[x,x+\\delta]\\cap Y\\in[c,d])}{P(X=[x,x+\\delta])} \\\\\\mathrm{d}y\n",
    "=& \\lim_{\\delta \\to 0} \\frac{\\int^d_c\\int^{x+\\delta}_x f_{XY}(x',y)\\mathrm{d}x'\\mathrm{d}y}{F_X(x+\\delta)-F_X(x)} \\\\\\mathrm{d}y\n",
    "=& \\lim_{\\delta \\to 0} \\int^d_c \\frac{\\frac{\\int^{x+\\delta}_{-\\infty} f_{XY}(x',y)\\mathrm{d}x' - \\int^x_{-\\infty} f_{XY}(x',y)\\mathrm{d}x'}{x+\\delta-x}}{\\frac{F_X(x+\\delta)-F_X(x)}{x+\\delta-x}} \\mathrm{d}y\\\\\\mathrm{d}y\n",
    "=& \\int^d_c \\frac{f_{XY}(x,y)}{f_X(x)} \\mathrm{d}y \\\\\\mathrm{d}y\n",
    "=& \\int^d_c f_{Y|X}(y|x)\\mathrm{d}y\n",
    "\\end{align*}\n",
    "$$\n",
    "> \n",
    "> From the fifth line to the sixth line, the fundamental theorem of calculus and the definition of derivative are invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-payment",
   "metadata": {},
   "source": [
    "## 2.3. Transformation of distribution\n",
    "\n",
    "Given the PDF $f_X(x)$ of a continuous random variable $X$, we can calculate the PDF of an associated random variable $Y=g(X)$ with a general formula:\n",
    "\n",
    "$$\n",
    "f_Y(y) = \\left|\\frac{f_X(x)}{g'(x)}\\right|_{x=g^{-1}(y)}\n",
    "$$\n",
    "\n",
    "for $y$ defined in the range of $g$. The function $g$ must be\n",
    "\n",
    "- continuous to possess a derivative $g'(y)=\\frac{\\mathrm{d}g}{\\mathrm{d}y}$ and\n",
    "- one-to-one to possess an inverse function $g^{-1}(y)$.\n",
    "\n",
    "The two conditions combine to imply that $g$ is strictly monotonic. If the function is not one-to-one, besides its lack of a proper inverse, its derivative hits zero somewhere (by Rolle's theorem) and invalidates the formula. Still, $f_Y(y)$ may be deduced with other approaches.\n",
    "\n",
    " \n",
    "> _(optional reading)_\n",
    ">\n",
    "> **Proof:** \n",
    "> \n",
    "> Let us first assume that $g$ is strictly increasing, i.e. $g'>0$.\n",
    ">\n",
    "> $$\n",
    "F_Y(y)\n",
    "=P(Y\\leq y)\n",
    "\\stackrel{g'>0}= P[X\\leq g^{-1}(y)]\n",
    "=F_X[g^{-1}(y)] \\ .\n",
    "$$\n",
    ">\n",
    "> Then by the chain rule,\n",
    ">\n",
    ">$$\n",
    "f_Y(y)\n",
    "=\\frac{\\mathrm{d}}{\\mathrm{d}y}F_X[g^{-1}(y)]\n",
    "=\\left.\\frac{\\mathrm{d}F_X}{\\mathrm{d}x}\\frac{\\mathrm{d}x}{\\mathrm{d}y}\\right|_{x=g^{-1}(y)}\n",
    "=\\left.\\frac{f_X(x)}{g'(x)}\\right|_{x=g^{-1}(y)} \\ .\n",
    "$$\n",
    ">\n",
    "> On the other hand, for a strictly decreasing $g$ with $g'<0$,\n",
    ">\n",
    "> $$\n",
    "\\begin{align*}\n",
    "F_Y(y)\n",
    "=& P(Y\\leq y)\n",
    "\\stackrel{g'<0}=P[(X\\geq g^{-1}(y))]\n",
    "=1-F_X[g^{-1}(y)] \\\\\n",
    "\\Rightarrow f_Y(y)=& -\\left.\\frac{f_X(x)}{g'(x)}\\right|_{x=g^{-1}(y)} \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    "> Our assumption $g'<0$ cancels the negative sign and again makes $f_Y(y)\\geq 0$. Hence, the two cases can be combined with an absolute sign.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-style",
   "metadata": {},
   "source": [
    "#### Example: Power function\n",
    "\n",
    "What are the PDFs of $Y=X^p$ and $Z=X^{-p}$ for $X\\sim U(0,1)$ with $p>0$?\n",
    "\n",
    "**Solution.** Because \n",
    "\n",
    "$$\n",
    "f_X(x)=\\begin{cases}\n",
    "1 & (0\\leq x\\leq1)\\\\\n",
    "0 & (\\text{otherwise})\\\\\n",
    "\\end{cases}\\ ,\n",
    "$$\n",
    "\n",
    "we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "f_Y(y) =&\n",
    "\\begin{cases}\n",
    "\\left|\\frac{1}{px^{p-1}}\\right| & (0\\leq y\\leq 1) \\\\\n",
    "0 & (\\text{otherwise})\n",
    "\\end{cases} \n",
    "=\\begin{cases}\n",
    "\\frac{1}{p}y^{1/p-1} & t(0\\leq y\\leq 1)\\\\\n",
    "0 & (\\text{otherwise})\n",
    "\\end{cases} \\\\[1em]\n",
    "f_Z(z) =& \\begin{cases}\n",
    "\\left|\\frac{1}{-px^{-p-1}}\\right| & (z\\geq 1 )\\\\\n",
    "0 & \\left(\\text{otherwise}\\right)\n",
    "\\end{cases}\n",
    "=\\begin{cases}\n",
    "\\frac{1}{p}z^{-1/p-1} & (z\\geq 1)\\\\\n",
    "0 & (\\text{otherwise})\n",
    "\\end{cases}\\ .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You may see that $f_Y(0)\\to\\infty$. Such a divergence does not invalidate $f_Y(y)$. (You may integrate it to see if it satisfies the normalization condition.) In fact, $F_Y(y)=\\sqrt{y}$, which indeed has a steep slope as $y\\to 0^+$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-links",
   "metadata": {},
   "source": [
    "### 2.4. Generation of random variables\n",
    "\n",
    "Most programming languages provide a random number generator that returns a uniform random variable $X\\sim U(0,1)$. We can generate random variables $Y$ with any strictly increasing CDF $F_Y(y)$ by defining\n",
    "\n",
    "$$\n",
    "Y=F_Y^{-1}(X) \\ .\n",
    "$$\n",
    "\n",
    "The monotonic condition is, again, necessary for $F_Y^{-1}$ to exist.\n",
    "\n",
    " \n",
    "> _(optional reading)_\n",
    ">\n",
    "> **Proof:**\n",
    ">\n",
    "> The CDF of $Y$ at a specific realization of $Y$ is also a random variable. Let us denote it with $\\tilde{Y}=F_Y(Y)$ and any specific realization of $\\tilde{Y}$ as $\\tilde{y}$. Now consider the new variable's CDF $F_\\tilde{Y}(\\tilde{y})$: for $\\tilde{y}\\in[0,1]$,\n",
    ">\n",
    "> $$\n",
    "F_\\tilde{Y}(\\tilde{y})\n",
    "= P(\\tilde{Y}\\leq \\tilde{y})\n",
    "= P[Y\\leq F_Y^{-1}(\\tilde{y})]\n",
    "= F_Y[F_Y^{-1}(\\tilde{y})] = \\tilde{y}\n",
    "$$\n",
    ">\n",
    "> and thus $f_\\tilde{Y}(y)=1$. Consequently, $\\tilde{Y}=F_Y(Y)\\sim U(0,1)$ is isomorphic to $X$, the (pseudo-)random number by our programs. Hence, $Y=F_Y^{-1}(X)$ has the desired CDF $F_Y(y)$.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-marketplace",
   "metadata": {},
   "source": [
    "#### Example 1: Linear mapping\n",
    "\n",
    "Express $Y\\sim U[a,b]$ in terms of $X\\sim U[0,1]$.\n",
    "\n",
    "**Solution.** For $y\\in[a,b]$, $F_Y(y)=\\frac{y-a}{b-a}$, so $Y=F_Y^{-1}(X)=(b-a)X+a$.\n",
    "\n",
    "#### Example 2: Logistic distribution\n",
    "\n",
    "Fig. 1 shows a logistic function $L(x)=\\frac{1}{1+e^{-x}}$. A distribution is logistic if its CDF is a logistic function. Express a logistically distributed random variable $Y$ with $F_y(y)=L(y)$ in terms of $X\\sim U[0,1]$.\n",
    " \n",
    "<figure style=\"text-align: center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" alt=\"logistic function\" style=\"width:50%\">\n",
    "    <figcaption> <b>Fig. 1</b> $L(x)=\\frac{1}{1+e^{-x}}$. Retrieved from <a href=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\">https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg</a>.</figcaption>\n",
    "</figure>\n",
    "    \n",
    "**Solution.** $Y=L^{-1}(X)=-\\ln\\left(\\frac{1}{X}-1\\right)$. You may verify this answer by plugging it into the general formula in Section 3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-diagram",
   "metadata": {},
   "source": [
    "# 3. Statistical measures\n",
    "\n",
    "## 3.1. Mean, variance and moments\n",
    "\n",
    "Moments are statistical quantities that describe a probabilistic distribution. For a random variable $X$ a probability density function (PDF) $f_X(x)$, the moments are defined as\n",
    "\n",
    "- N-th moment: $$\\langle X^n\\rangle = \\int^\\infty_{-\\infty} x^n f_X(x) \\mathrm{d}x$$\n",
    "\n",
    "- N-th central moment: $$\\langle (X-\\langle X\\rangle)^n\\rangle = \\int^\\infty_{-\\infty} (x-\\langle X\\rangle)^n f_X(x) \\mathrm{d}x$$\n",
    "\n",
    "The idea of moment is generalizing from some common statistical quantities\n",
    "\n",
    "- Mean = The first moment. Usually denoted as $\\langle X\\rangle$.\n",
    "\n",
    "- Variance = the second _central_ moment. Usually denoted as $\\sigma^2$.\n",
    "\n",
    "$$\\sigma^2 = \\langle (X-\\langle X\\rangle)^2\\rangle = \\langle X^2 \\rangle -\\langle X\\rangle ^2$$\n",
    "\n",
    "- skewness, kurtosis = third and forth central moment after standardization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-shore",
   "metadata": {},
   "source": [
    "## 3.2. Characteristic function\n",
    "\n",
    "The characteristic function of a probabilistic distribution is its expecation of $e^{ikx}$, i.e. \n",
    "\n",
    "$$\n",
    "M_X(k) = \\langle e^{ikX}\\rangle = \\int^\\infty_{-\\infty} e^{ikx} f_X(x)\\mathrm{d}x\n",
    "$$\n",
    "\n",
    "Be careful that the characteristic is a complex-value function. There are various applications:\n",
    "\n",
    "- An alternative way for convolution: \n",
    "\n",
    "    Let $X$ and $Y$ be two _independent_ random variables with PDF $f_X(x)$ and $g_Y(y)$, the PDF of the variable $Z=X+Y$ can be found by \n",
    " \n",
    "    - Method 1: direct calculation of the convolution integral:\n",
    "     $$ \n",
    "     h_Z(z) = \\int^\\infty_{-\\infty} f_X(x)g_Y(z-x) \\mathrm{d}x\n",
    "     $$\n",
    "    \n",
    "    - Method 2: First find the the characteristic function of $X$ and $Y$, and then calculate the characteristic function of $Z=X+Y$ by \n",
    "\n",
    "     $$ \n",
    "    M_Z(k) = \\langle e^{ikZ} \\rangle = \\langle e^{ik(X+Y)} \\rangle = \\langle e^{ikX}e^{ikY} \\rangle = \\langle e^{ikX} \\rangle\\langle e^{ikY} \\rangle = M_X(k)M_Y(k)\n",
    "    $$\n",
    "\n",
    "     Then convert back to the original probabilistic function from the characteristic function, simple apply the inverse Fourier transform.\n",
    "\n",
    "     $$\n",
    "    h_Z(z) = \\frac{1}{2\\pi}\\int^\\infty_{-\\infty} e^{-ikz} M_Z(k) \\mathrm{d}k\n",
    "    $$\n",
    "\n",
    "\n",
    "- Moment generating function:\n",
    "\n",
    " The moments of a distribution can be calculated by the general formula over characteristic function:\n",
    " \n",
    " $$ \n",
    " \\langle X^n\\rangle = \\frac{1}{i^n}\\left.\\frac{\\mathrm{d}^nM_X(k)}{\\mathrm{d}^nk}\\right|_{k=0} \n",
    "$$\n",
    " \n",
    " > _(Optional reading)_\n",
    " > \n",
    " > **Proof:**\n",
    " > Making use of the Taylor expansion:\n",
    " > $$\\begin{align*} \n",
    " e^{ikx} &= 1+ (ikx) + \\frac{(ikx)^2}{2!} + \\frac{(ikx)^3}{3!} + ...\\\\\n",
    " \\left.\\frac{\\mathrm{d}^n}{\\mathrm{d}^nk}e^{ikx}\\right|_{k=0} &= (ix)^n \\\\\n",
    " \\int^\\infty_{-\\infty} \\left[\\left.\\frac{\\mathrm{d}^n}{\\mathrm{d}^nk}e^{ikx}\\right|_{k=0}\\right] f_X(x)\\mathrm{d}x &= \\int^\\infty_{-\\infty} \\left[(ix)^n\\right] f_X(x) \\mathrm{d}x \\\\\n",
    " \\left.\\frac{\\mathrm{d}^n}{\\mathrm{d}^nk} \\left(\\int^\\infty_{-\\infty} e^{ikx}f_X(x)\\mathrm{d}x\\right) \\right|_{k=0} &= i^n \\int^\\infty_{-\\infty} x^n f_X(x)\\mathrm{d}x \\\\\n",
    " \\left.\\frac{\\mathrm{d}^nM_X(k)}{\\mathrm{d}^nk}\\right|_{k=0} &= i^n \\langle X^n \\rangle\n",
    " \\end{align*}\n",
    " $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-cowboy",
   "metadata": {},
   "source": [
    "## 3.3. Covariance and correlation\n",
    "\n",
    "- The **covariance** between two random variables $X$ and $Y$ reads \n",
    "\n",
    " $$\n",
    "\\begin{align*}\n",
    "\\sigma_{XY}^2 &= \\langle (X-\\langle X\\rangle)(Y-\\langle Y \\rangle)\\rangle \\\\\n",
    "&= \\langle XY \\rangle - \\langle X\\rangle \\langle Y \\rangle \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " It gets this name for its identical form to variance; if $Y=X$, $\\sigma_{XY}^2 = \\sigma_X^2$. Two variables has a positive covariance if an increase in one often occurs with an increase in the other. On the other hand, two variables has a more negative covariance if they possess opposite trends.\n",
    "\n",
    "- The **correlation** (or correlation coefficient) is the normalized covariance for comparing behaviours of various pirs of variables. The correlation between $X$ and $Y$ is canonically defined as \n",
    "\n",
    " $$\n",
    "r_{XY} = \\frac{\\sigma^2_{XY}}{\\sigma_X \\sigma_Y} \\in [-1,1]\n",
    "$$\n",
    " \n",
    " which may be specifically called Pearson's correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-england",
   "metadata": {},
   "source": [
    "### 3.3.1. Correlation verus dependence\n",
    "\n",
    "The Pearson's correlation between two variables only indicates the strength of their linear dependence. In other words, a higher correlation between two variables means their scatter plot resemble a straight line more **regardless of its slope**. \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg\" alt=\"Correlation plot\" style=\"width:50%\">\n",
    "    <figcaption style=\"text-align: left\"> <b>Fig. 2</b> The number above each subplot indicates the correlation between the horizontal and the vertical variables. (Retrieved from <a href=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg\"> https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg</a>.) For the central subplot, correlation is undefined for the zero variance of its vertical variable.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-marshall",
   "metadata": {},
   "source": [
    "#### Example: quadratic dependence\n",
    "\n",
    "Let a random variable $X \\sim U(-1,1)$, i.e. $X$ is uniformly distributed in $[-1,1]$. What is its correlation between $Y=X^2$?\n",
    "\n",
    "**Solution**. Because of its uniform distribution, the PDF of $X$ is\n",
    "\n",
    "$$\n",
    "f_X(x) = \\begin{cases}\n",
    "\\frac{1}{2} & (-1\\leq x\\leq 1) \\\\\n",
    "0 & (\\text{otherwise})\n",
    "\\end{cases}\\ .\n",
    "$$\n",
    "\n",
    "Then we can compute $\\langle X\\rangle$, which we need for $\\sigma^2_{XY} = \\langle XY\\rangle -\\langle X\\rangle \\langle Y\\rangle = \\langle X^3\\rangle - \\langle X \\rangle \\langle X^2 \\rangle$ :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle X \\rangle &= \\int^\\infty_{-\\infty} xf_X(x)\\, dx \\\\\n",
    "&= \\frac{1}{2} \\int^1_{-1} x\\, dx \\\\\n",
    "&= 0 \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similarly, $\\langle X^3\\rangle$, so does $\\sigma^2_{XY}$. This results implies that the correlation between $X$ and $Y$ is $r_{xy} = 0$, and it ultimately teaches us that Peason's correlation does not measure the strength of nonlinear dependence reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-german",
   "metadata": {},
   "source": [
    "### 3.3.2. Rank correlation (Optional reading)\n",
    "\n",
    "Two useful alternatives to Person's correlation are Spearman's correlation and Kendall's correlation, which are special cases of rank correlation. This class of correlation is devised to respond more sensitively to nonlinear dependence. \n",
    "\n",
    "Unlike Person's correlation, rank correlation usually requires an explicit knowledge of observed data, so let us first assume there are $n$ realization of $(X,Y)$, i.e. $\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$. Rank correlation first transforms each $(x_i, y_i)$ to a rank variable $(R_{x_i}, R_{y_i})$, where $R_{x_i} = k$ if $x_i$ is the k-th smallest realization of $X$.\n",
    "\n",
    "- **Spearman's correlation**. Also called Spearman's $\\rho$, it is in fact the Pearson's correlation between $R_x = \\{R_{x_i}\\}$ and $_Y = \\{R_{y_i}\\}$, so \n",
    "\n",
    "$$\n",
    "\\rho_{XY} = \\frac{\\sigma^2_{R_X R_Y}}{\\sigma_{R_X}\\sigma_{R_Y}}\\ .\n",
    "$$\n",
    "\n",
    "- **Kendall's correlation**. Also called Kendall's $\\tau$, it firsts assigns an $x$-score $\\hat{x}_{ij} = \\text{sgn}(R_{x_i}-R{x_j})$ and a $y$-score $\\hat{y}_{ij} = \\text{sgn}(R_{y_i}-R{y_j})$ to each pair of $(R_{x_i}, R_{y_i})$ and $(R_{x_j}, R_{y_j})$ with $i<j$. Then the correlation is defined as \n",
    "\n",
    "$$\n",
    "\\tau_{XY} = \\frac{2}{n(n-1)}\\sum_{i<j} \\hat{x}_{ij}\\hat{y}_{ij}\\ .\n",
    "$$\n",
    "\n",
    "In Kendall's original terms, a pair is concordant if $\\hat{x}_{ij}\\hat{y}_{ij}>0$ but discordant if $\\hat{x}_{ij}\\hat{y}_{ij}<0$, whears it is neighter concordant nor disccordant if $\\hat{x}_{ij}\\hat{y}_{ij}=0$. This trichotomy helps reformulate Kendall's correlation as \n",
    "\n",
    "$$\n",
    "\\tau_{XY} = \\frac{\\text{#concordances}-\\text{#discordances}}{n(n-1)/2}\\ .\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
