{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "herbal-association",
   "metadata": {},
   "source": [
    "# MSDM5058 Tutorial 1 (Part 2) - Time Series Analysis\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Time series\n",
    "2. Moving average\n",
    "3. Prediction with MACD\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-brain",
   "metadata": {},
   "source": [
    "# 1. Time series\n",
    "\n",
    "A time series is a series of data points indexed in time order. You may find various kinds of notation to denote a time series from different sources, e.g. $X_t$ and $X(t)$. \n",
    "\n",
    "The \"time\" of a time series may be either distrete or continuous. Their main difference comes from the evaluation of angle brackets (i.e. when you need to find some average quantities):\n",
    "\n",
    "- Discrete = Replace by summation, i.e. $\\langle X_t X_{t+\\tau}\\rangle \\sim \\sum X_t X_{t+\\tau}$\n",
    "\n",
    "- Continuous = Replace by integration, i.e. $\\langle X_t X_{t+\\tau}\\rangle \\sim \\int X_t X_{t+\\tau} \\mathrm{d}t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-clothing",
   "metadata": {},
   "source": [
    "## 1.1. Sample mean and variance\n",
    "\n",
    "A discrete time series usually comes from sampling data from an infinitely long series. The mean and the variance of are, strictly speaking, not possible to evalulate unless we can wait infinitely long to sample get all data,  which is impractical. Instead, we can use the sampling measures:\n",
    "\n",
    "- Sample mean: $$\\hat{\\mu}_X = \\langle X_t\\rangle = \\frac{1}{T}\\sum_{t=1}^T X_t$$\n",
    "\n",
    "- Sample variance: $$\\hat{\\sigma}^2_X = \\langle (X_t-\\hat{\\mu}_X^2 \\rangle = \\frac{1}{T-1}\\sum_{t=1}^T(X_t-\\hat{\\mu})^2$$\n",
    "\n",
    "\n",
    "On the other hand, continuous time series usually comes from a well-defined function, so , we can evaluate the integrals to infinity:\n",
    "\n",
    "- Mean: $$\\mu_X = \\langle X_t \\rangle = \\lim_{T\\to\\infty} \\frac{1}{2T}\\int^T_{-T} X_t \\mathrm{d}t $$\n",
    "\n",
    "- Variance: $$\\sigma^2_X = \\langle (X_t-\\mu_X)^2\\rangle = \\lim_{T\\to\\infty} \\frac{1}{2T}\\int^T_{-T} (X_t-\\mu)^2\\mathrm{d}t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-timing",
   "metadata": {},
   "source": [
    "## 1.2. Autocorrelation\n",
    "\n",
    "The Autocorrelation of with a lag $\\tau$ is defined as \n",
    "\n",
    "$$\n",
    "R_X(\\tau) = \\langle X_t X_{t+\\tau} \\rangle = \n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac{1}{T}\\sum_{t=1}^{T-\\tau}X_t X_{t+\\tau} &\\text{(discrete)}\\\\\n",
    "\\displaystyle \\lim_{T\\to\\infty} \\frac{1}{2T}\\int^T_{-T} X_t X_{t+\\tau}\\mathrm{d}t &\\text{(continouous)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Intuitively it indicates the correlation of data separated $\\tau$ units apart. If $X_t$ and $X_{t+\\tau}$ are often both high or low, $R_X(\\tau)$ is high. (\"Auto-correlation\" literally means the correlation with one's self.) \n",
    "\n",
    "- **Note 1:** One should notice that the normalization constant for the discrete case is taken as $T$ instead of $T-\\tau$, although there are $T-\\tau$ terms in the summation. The justification is that \n",
    "\n",
    "    - For a long-term correlation with $\\tau \\approx T$, the summation only has a few terms and thus becomes highly prone to noise, which needs to be suppressed by a harder normalization constant $\\frac{1}{T}$. \n",
    "    \n",
    "    - For a short-term correlation with $\\tau \\ll T$, $\\frac{1}{T}$ barely differs from $\\frac{1}{T-\\tau}$. \n",
    "\n",
    "\n",
    "\n",
    "- **Note 2:** Since \"high\"and \"low\" are usually relative to the mean $\\mu_X$ of the series, one may redefine autocorrelation by \n",
    "\n",
    " $$\n",
    "\\begin{align*}\n",
    "K_X(\\tau) &= \\langle (X_t-\\mu_X)(X_{t+\\tau}-\\mu_X)\\rangle \\\\\n",
    "&= \\langle X_t X_{t+\\tau} \\rangle - \\mu_X^2 \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " Here comes a trouble in naming: while physicists prefer still calling it \"autocorrelation\", statiscians prefer calling this centralized moment as \"**autocovariance**\" instead. Worse still, the **autocorrelation coefficient** of $X$ is indisputably always defined as \n",
    "\n",
    " $$\n",
    "\\rho_X(\\tau) = \\frac{K_X(\\tau)}{\\sigma_X^2}\\ ,\n",
    "$$\n",
    "\n",
    " where $\\sigma^2_X$ is its variance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-davis",
   "metadata": {},
   "source": [
    "#### Example: A cosine signal\n",
    "\n",
    "What is the autorrelation of an infinite continuous-time cosine signal $X(t) = A\\cos(\\omega t+\\phi) + c$ with $ t \\in [0,\\infty)$? (Mathematicians may call this domain semi-finite.)\n",
    "\n",
    "**Solution**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R_X(\\tau) = \\langle A^2 \\cos(\\omega t+\\phi)\\cos(\\omega t +\\omega \\tau+\\phi) + Ac\\cos(\\omega t+\\phi) + Ac\\cos(\\omega t + \\omega\\tau + \\phi) + c^2 \\rangle\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "1. By product-to-sum formula, the first term equals \n",
    "\n",
    " $$\n",
    "\\lim_{T\\to 0} \\left[\\frac{A^2}{2T}\\int^T_0 \\cos(\\omega t)\\, dt+ \\frac{A^2}{2T}\\int^T_0\\cos(2\\omega t+\\omega\\tau+2\\phi)\\, dt\\right] = \\frac{A^2}{2}\\cos(\\omega \\tau)\\ .\n",
    "$$\n",
    "\n",
    "2. The second term equals \n",
    "\n",
    "$$\n",
    "\\lim_{T\\to\\infty} \\frac{Ac}{T}\\int^T_0 \\cos(\\omega t+\\phi)\\, dt = \\lim_{T\\to\\infty} \\frac{Ac}{\\omega T}[\\sin(\\omega T+\\phi)-\\sin\\phi] = 0\\ .\n",
    "$$\n",
    "\n",
    " As the difference between 2 sine functions is bounded within $[-2,2]$, the limit vanishes due to the presence of $\\frac{1}{T}$\n",
    "\n",
    "3. The third term is similar to the second term. The limit also vanishes. \n",
    "\n",
    "Finally, $$R_X(\\tau) = \\frac{A^2}{2}\\cos(\\omega\\tau) + c^2$$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-paris",
   "metadata": {},
   "source": [
    "# 2. Moving average\n",
    "\n",
    "It is often assumed that a time series $X_t = \\hat{X}_t + \\varepsilon_t$ consists of an underlying pattern $\\hat{X}$ and some random noise $\\varepsilon$, where we expect $\\langle \\varepsilon \\rangle \\to 0$. While noise distorts a time series, **moving average** helps attenuate noise and thus reveal its underlying pattern. There are various kinds of moving averages. I am going to discuss two: **simple moving average** and **exponential moving average**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-glance",
   "metadata": {},
   "source": [
    "## 2.1. Simple moving average\n",
    "\n",
    "The simple moving average (SMA) of $X$ reads \n",
    "\n",
    "$$\n",
    "\\text{SMA}_X(t;\\omega) = \\frac{1}{\\omega}\\sum_{i=0}^{\\omega-1}X_{t-i}\\ ,\n",
    "$$\n",
    "\n",
    "where the parameter $\\omega$ corresponds to a **window** size. SMA average each data point out with ones that are at most $\\omega-1$ steps earlier. (The $\\omega$ points are thus said to be in the same window.) In this way, SMA helps smooth out noise with a period shorter that $\\omega$. For example, a sinusoidal noise $\\epsilon_t=\\sin t$ disappears in a window with $\\omega=2\\pi$. However, a prolonged window blurs the underlying pattern as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-lover",
   "metadata": {},
   "source": [
    "## 2.2. Exponential moving average\n",
    "\n",
    "The spirit of exponential average (EMA) is to **emphasize new data more than old data**. Just imagine a stock: everyone expects its price today to be received a stronger influence from yesterday's headlines than a finanicial crisis decades ago. Given this spirit, there comes two definitions of EMA. \n",
    "\n",
    "- The definition that you learn from the lecture is\n",
    "\n",
    " $$\n",
    "\\text{EMA}_X^L(t;\\omega,\\tau) = \\frac{1}{Z_{\\omega,\\tau}}\\sum_{i=1}^{\\omega-1}X_{t-i}e^{-i/\\tau}\n",
    "$$\n",
    "\n",
    " with a window size $\\omega$, a memory length $\\tau \\in (0, \\infty)$, and a normalization constant $Z_{\\omega,\\tau}=\\sum_{i=0}^{\\omega-1} e^{-i/\\tau}$. A longer $\\tau$ values old data within a window more, $\\tau\\to\\infty$ reduces it to the SMA. \n",
    "\n",
    "- Another definition is recursive:\n",
    "\n",
    " $$\n",
    "\\begin{align*}\n",
    "\\text{EMA}_X^R(t;a) &= (1-a)\\text{EMA}_X(t-1;a)+aX_{t-1} \\\\\n",
    "&= a\\sum_{i=0}^{t-1}X_{i-1}(1-a)^i\\ ,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " where $a\\in[0,1]$ is some attenuation factor. A larger $a$ puts a greater emphasis on new data. This definition is more prevalent, especially in finance, mainly for its simpler definition and more efficient computation: given the EMAs at all previous instants, $\\text{EMA}^R$ costs $O(1)$ to compute the current EMA, but $\\text{EMA}^L$ costs $O(\\omega)$.\n",
    "\n",
    "Still, techniqually speaking, only $\\text{EMA}^L$ belongs to moving average, while $\\text{EMA}^R$ is classified as **autoregression**. This dichotomy arises because they treat data in fundamentally distinct ways. Supposing $X$ jumps sharply at $t=1$ due to an impulse $\\varepsilon_t=\\delta(t-1)$,\n",
    "\n",
    "- In $\\text{EMA}^L$, the impulse no longer has influence once it leaves the window, i.e. if $t>1+\\omega$.\n",
    "\n",
    "- In $\\text{EMA}^R$, the remnant of the impulse will stay forever, however small it is. \n",
    "\n",
    "(Therefore engineers alternatively name them as a finite-impulse-response filter and an infinite-impulse-response filter.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-marketplace",
   "metadata": {},
   "source": [
    "### Simplifying the notation of EMA\n",
    "\n",
    "Before proceeding, let's simplify the notation of EMA: For an $n$-unit EMA of $X$, now denoted as $\\text{EMA}_X{\\left(t;n\\right)}$ without any superscript, corresponds to either definition:\n",
    "\n",
    "- With $\\text{EMA}^\\text{L}$, $w=n$, while $\\tau$ is undetermined.\n",
    "\n",
    "- With $\\text{EMA}^\\text{R}$, the natural choice is $a=\\frac{1}{n}$, which suppresses a datum's weight by $\\frac{1}{e}$ every $n$ units, but a more common choice turns out to be $a=\\frac{2}{n+1}$. (You may read [Wikipedia](https://en.wikipedia.org/wiki/Moving_average#Relationship_between_SMA_and_EMA) for its derivation.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-february",
   "metadata": {},
   "source": [
    "# 3. Prediction with MACD\n",
    "\n",
    "**MACD**, standing for **moving-average-convergence-divergence**, is a tool for predicting the future trend of a time series. If $X$ represents the time series of a stock's daily price, its MACD is defined by subtracting its 26-day EMA from its 12-day EMA :\n",
    "\n",
    "$$\n",
    "M_X(t) = \\text{EMA}_X(t;12) - \\text{EMA}_X(t;26) \\ .\n",
    "$$\n",
    "\n",
    "Then its corresponding **signal line** (or called trigger line) is given as its 9-day EMA:\n",
    "\n",
    "$$\n",
    "S_X(t) = \\text{EMA}_{M_X}(t; 9) \\ .\n",
    "$$\n",
    "\n",
    "The three periods (9 days, 12 days, and 26 days) are purely **_conventional_**. They are thus chosen just because they correspond to one and a half weeks, two weeks, and one month in a market with six working days weekly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-bowling",
   "metadata": {},
   "source": [
    "## 3.1. Interpretation\n",
    "\n",
    "Traders usually interprets the MACD of a stock's price $X$ from three aspects. In their jargon, \"bull-ish\" and \"bear-ish\" means an increasing trend and a decreasing trend respectively.\n",
    "\n",
    " \n",
    "\n",
    "- **Signal-line crossover**\n",
    "    \n",
    "    - When $M_X(t)$ breaks above $S_X(t)$, there is a strong bull-ish sign, suggesting that $X_t$ may rise rapidly. \n",
    "    \n",
    "    - When $M_X(t)$ breaks below $S_X(t)$, there is a strong bear-ish sign, suggesting that $X_t$ may drop rapidly.\n",
    "\n",
    "- **Zero crossover**\n",
    "\n",
    "    - When $M_X(t)$ breaks above zero, there is a mild bullish sign, suggesting that $X_t$ may rise moderately. (This is also called a golden cross.) \n",
    "    \n",
    "    - When $M_X(t)$ breaks below zero, there is a mild bearish sign, suggesting that the $X_t$ may drop moderately. (This is also called a death cross.)\n",
    "\n",
    "- **Divergence**\n",
    "\n",
    "    - When $X_t$ drops to a new low but $M_X(t)$ does not, there is a sign of bullish divergence, suggesting that $X_t$ may stop dropping. \n",
    "    \n",
    "    - When $X_t$ rises to a new high but $M_X(t)$ does not, there is a sign of bearish divergence, suggesting that $X_t$ may stop rising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-wound",
   "metadata": {},
   "source": [
    "## 3.2. Why does MACD work?\n",
    "\n",
    "Traders predicts with MACD because it works empirically, but why? Loosely speaking, $M_X(t)\\sim\\frac{d X}{dt}$. First recall that for a function $f$, its derivative $\\frac{df}{dt} \\approx \\frac{f(t_2)-f(t_1)}{t_2-t_1}$. Now compare this with the definition of $M_X(t)$: it subtracts a long-term average from a short-term average, and each average approximates a value of the underlying pattern $\\tilde{X}$. It implies that (to be more general, let us replace 12 and 26 with $\\tau_\\text{short}$ and $\\tau_\\text{long}$)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "M_X(t)&\\approx \\tilde{X}_{t-\\tau_\\text{short}} - \\tilde{X}_{t-\\tau_\\text{long}} \\\\\n",
    "&\\approx (\\tau_\\text{long}-\\tau_\\text{short}) \\frac{d\\tilde{X}}{dt} \\ .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore $M_X(t)$ can be used to detect $\\frac{d X}{dt}$, which equals $\\frac{d \\tilde{X}}{dt}$ on average. A similar argument suggests that $M_X(t)- S_X(t) \\sim \\frac{d^2 X}{dt^2}$.\n",
    "\n",
    "To conclude with an analogy, if $X$ described a random walker's displacement, $M_X(t)$ and $M_X(t)- S_X(t)$ would model the walker's velocity and acceleration. (If you want to go through a more rigorous discussion, please visit this [webpage](https://gregstanleyandassociates.com/whitepapers/FaultDiagnosis/Filtering/MACD-approach/macd-approach.htm).)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
